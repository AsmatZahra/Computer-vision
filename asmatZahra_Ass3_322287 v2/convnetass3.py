# -*- coding: utf-8 -*-
"""convNetAss3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14yJjJuGJ7te3zBdTxKCchlZMdyScjNek

# 1.Mount the drive
"""

from google.colab import drive
drive.mount('/content/drive')

"""# 2.Change of Working directory and Clone Git hub"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/My Drive/Colab Notebooks
!git clone https://github.com/engrhamzaaliimran/cvassignmentdataset.git

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/My Drive/Colab Notebooks/cvassignmentdataset
!ls

"""# 3.Split into train test"""

from keras.preprocessing.image import ImageDataGenerator
datagen = ImageDataGenerator()
train_dir= '/content/drive/My Drive/Colab Notebooks/cvassignmentdataset/Cyclone_Wildfire_Flood_Earthquake_Database/small_dataset/train'
test_dir= '/content/drive/My Drive/Colab Notebooks/cvassignmentdataset/Cyclone_Wildfire_Flood_Earthquake_Database/small_dataset/test'
validation_dir= '/content/drive/My Drive/Colab Notebooks/cvassignmentdataset/Cyclone_Wildfire_Flood_Earthquake_Database/small_dataset/validation'
 #load and iterate training dataset
train_it = datagen.flow_from_directory(train_dir, class_mode='categorical', batch_size=20)
# load and iterate validation dataset
val_it = datagen.flow_from_directory(validation_dir, class_mode='categorical', batch_size=20)
# load and iterate test dataset
test_it = datagen.flow_from_directory(test_dir, class_mode='categorical', batch_size=20)

"""# 6. Generators each for testing, training and validation"""

from keras.preprocessing.image import ImageDataGenerator

# All images will be rescaled by 1./255
train_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)
validation_datagen= ImageDataGenerator(rescale= 1./255)

train_generator = train_datagen.flow_from_directory(
        # This is the target directory
        train_dir,
        # All images will be resized to 150x150
        target_size=(125, 125),
        batch_size=50,
        # we need categorical labels
        class_mode= 'categorical',
        shuffle=True)
test_generator = test_datagen.flow_from_directory(
        test_dir,
        target_size= (125,125),
        batch_size=50,
        class_mode ='categorical',
        shuffle=False)
validation_generator = validation_datagen.flow_from_directory(
        validation_dir,
        target_size=(125, 125),
        batch_size=50,
        class_mode='categorical',
        shuffle=True)

#Another generator with missing batch size

from keras.preprocessing.image import ImageDataGenerator

# All images will be rescaled by 1./255
train_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)
validation_datagen= ImageDataGenerator(rescale= 1./255)

train_generator = train_datagen.flow_from_directory(
        # This is the target directory
        train_dir,
        # All images will be resized to 150x150
        target_size=(125, 125),
      
        # we need categorical labels
        class_mode= 'categorical',
        shuffle=True)
test_generator = test_datagen.flow_from_directory(
        test_dir,
        target_size= (125,125),
        
        class_mode ='categorical',
        shuffle=False)
validation_generator = validation_datagen.flow_from_directory(
        validation_dir,
        target_size=(125, 125),
        
        class_mode='categorical',
        shuffle=True)

"""# 4. Building Conv Model"""

from keras import layers
from keras import models
from keras.layers import Dense, Dropout, Flatten
model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(125, 125, 3)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(128, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(128, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(512, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Flatten())
model.add(layers.Dense(512, activation='relu'))
model.add(Dropout(0.3))
model.add(layers.Dense(4, activation='softmax'))

model.summary()

"""# 5. Optimizer Selection

# ***TENSOR BOARD***

# 7. Fitting Model
"""

# Commented out IPython magic to ensure Python compatibility.
from keras import optimizers

model.compile( loss= 'categorical_crossentropy',
              optimizer= optimizers.Adamax(),
              metrics=['accuracy'])
##############TENSOR BOARD################
import tensorflow as tf
import os
import datetime
# %reload_ext tensorboard

#tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)
logdir = os.path.join("logs", datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=0,write_graph=True)
#############FITTING MODEL#############################
history = model.fit_generator(
      train_generator,
      steps_per_epoch=100,
      epochs=10,

      validation_data=validation_generator,
      validation_steps=50)

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir logs

"""# 8.Getting Results"""

loss = model.evaluate_generator(test_generator, steps=20)
print (loss)

X_test=test_generator
y_pred = model.predict_generator(X_test)
y_pred=np.argmax(y_pred,axis=1)

#valid_labels= validation_generator.classes
true_test_labels= test_generator.classes
pred_test_labels= y_pred
#labels = (train_generator.class_indices)
#print (len(validation_labels))
#print (len(test_labels))
#print (true_test_labels)
#print (valid_labels)

from sklearn.metrics import confusion_matrix
cm = sklearn.metrics.confusion_matrix(true_test_labels, pred_test_labels)
#cm = plot_confusion_matrix(y_true, y_pred)
print (cm)

"""# **Plotting Confusion Matrix**"""

from sklearn.metrics import confusion_matrix
import seaborn as sns
target_names = ['cyclone', 'earthquake', 'flood', 'wildfire']

# Normalise
cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
fig, ax = plt.subplots(figsize=(5,5))
sns.heatmap(cm_norm, annot=True, fmt='.2f', xticklabels=target_names, yticklabels=target_names)
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show(block=False)

from sklearn.metrics import classification_report
#y_true = [0, 1, 2, 2, 2]
 #y_pred = [0, 0, 2, 2, 1]
y_true = validation_labels

target_names = ['cyclone', 'earthquake', 'flood', 'wildfire']
print(classification_report(y_true, y_pred, target_names=target_names))

import matplotlib.pyplot as plt

acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

